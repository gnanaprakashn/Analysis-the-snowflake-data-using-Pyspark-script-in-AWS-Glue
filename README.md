
# â„ï¸ Snowflake Data Analysis using PySpark in AWS Glue

This project demonstrates how to integrate **Snowflake** with **Apache Spark (PySpark)** inside **AWS Glue** for performing data extraction, transformation, and loading (ETL).  
It uses JDBC and Spark-Snowflake connectors to enable bidirectional data movement between Snowflake and Glue.

This setup is commonly used by data engineering teams to process Snowflake data at scale using distributed Spark jobs.

---

## ğŸ—ï¸ Architecture


![Snowflake Glue Architecture](glue job to snowflake.png)
<img src="glue job to snowflake.png" width="850"/>

**Flow Overview:**  
AWS Glue (PySpark) â†’ Snowflake (via Spark Connector + JDBC) â†’ Transform â†’ Write Back

---

## ğŸš€ What This Pipeline Does (Simple Explanation)

1. AWS Glue job runs a **PySpark script**  
2. Spark uses:
   - **Snowflake JDBC driver**  
   - **Snowflake Spark Connector**  
3. Reads data from Snowflake tables  
4. Runs transformations / SQL queries using Spark  
5. Writes processed results back into Snowflake  

This enables scalable Snowflake analytics using Spark clusters.

---

## ğŸ“ Repository Structure

```

Snowflake-Data-Spark-Integration/
â”‚
â”œâ”€â”€ gluejobforsnowflake.py                  # Main PySpark ETL script for Glue
â”œâ”€â”€ snowflake-jdbc-3.13.15.jar              # JDBC connector for Snowflake
â”œâ”€â”€ spark-snowflake_2.12-2.9.2-spark_3.1.jar # Spark-Snowflake connector
â”œâ”€â”€ glue job to snowflake.png               # Architecture Diagram
â”œâ”€â”€ read me..txt                            # Notes / reference
â””â”€â”€ README.md

````

---

## ğŸ”§ Components Used

### **1ï¸âƒ£ Snowflake JDBC Connector**
- File: `snowflake-jdbc-3.13.15.jar`  
- Purpose: Enables low-level JDBC connectivity between Spark and Snowflake.

### **2ï¸âƒ£ Snowflake Spark Connector**
- File: `spark-snowflake_2.12-2.9.2-spark_3.1.jar`  
- Purpose: Allows Spark to read/write Snowflake tables using Spark DataFrames efficiently.

### **3ï¸âƒ£ PySpark Script (AWS Glue Job)**
- File: `gluejobforsnowflake.py`  
- Purpose: Executes Spark job to:
  - Read tables  
  - Run SQL  
  - Write processed output back to Snowflake  

---

# ğŸ§  How the ETL Works

### **Step 1 â€” Initialize Spark Session**
AWS Glue starts a distributed PySpark session.

### **Step 2 â€” Load Snowflake Connection Options**
Includes:
- URL  
- Username  
- Password  
- Database  
- Schema  
- Warehouse  

### **Step 3 â€” Read Data from Snowflake**

```python
df = spark.read.format("net.snowflake.spark.snowflake")
````

### **Step 4 â€” Transformations / SQL Queries**

Spark executes aggregations, filtering, joins, and SQL operations.

### **Step 5 â€” Write Back to Snowflake**

```python
df.write.mode("overwrite").format("snowflake")
```

---

# ğŸ“œ PySpark Code (Main Logic Overview)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Snowflake Integration").getOrCreate()

SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"

snowflake_database = ""
snowflake_schema = ""
snowflake_table_name = ""

snowflake_option = {
    "sfUrl": "",
    "sfUser": "",
    "sfPassword": "",
    "sfdatabase": snowflake_database,
    "sfschema": snowflake_schema,
    "sfWarehouse": ""
}

# Read table
df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \
    .option(**snowflake_option) \
    .option("dbtable", snowflake_table_name) \
    .load()

# Perform SQL or transformations
sql_query = """ 
SELECT * FROM table 
"""

# Write back
df.write.format("snowflake") \
    .option(**snowflake_option) \
    .option("dbtable", "output") \
    .mode("overwrite") \
    .save()

spark.stop()
```

---

# âš™ï¸ How to Run This in AWS Glue

### **1ï¸âƒ£ Upload Connectors**

Upload to S3 and attach to Glue job:

* `snowflake-jdbc-3.13.15.jar`
* `spark-snowflake_2.12-2.9.2-spark_3.1.jar`

### **2ï¸âƒ£ Create AWS Glue Job**

Configure:

* Script: `gluejobforsnowflake.py`
* Worker type: Standard / G.1X
* Glue version: Spark 3.x
* IAM role: S3 + Snowflake permissions

### **3ï¸âƒ£ Update Script with Snowflake Credentials**

Set:

* URL
* USER
* PASSWORD
* DATABASE
* SCHEMA
* WAREHOUSE

### **4ï¸âƒ£ Run the Job**

It will:

* Connect to Snowflake
* Read table
* Process data
* Save results back

---

# ğŸ§ª Testing & Validation

### âœ” Check connector loading in Glue logs

### âœ” Run Snowflake read query

```sql
SELECT * FROM MY_TABLE;
```

### âœ” Verify processed output table

### âœ” Review Glue logs for ETL success/failure

---

# ğŸ¯ Skills Demonstrated

* Snowflake + Spark integration
* Distributed ETL using AWS Glue
* Connector-based data ingestion
* Secure external database access
* Writing DataFrames back to Snowflake
* Cloud-based data engineering patterns

---

# ğŸ“„ Resume Bullet Points

* Built an ETL pipeline using **AWS Glue PySpark** to read and write Snowflake datasets using JDBC and Spark-Snowflake connectors.
* Executed distributed SQL and DataFrame transformations at scale.
* Implemented secure connection handling and optimized Spark configuration for Snowflake workloads.

---

# ğŸ‘¤ Author

**Gnana Prakash N**
Data Engineer
GitHub: [gnanaprakashn](https://github.com/gnanaprakashn)

---

# ğŸ“œ License

MIT Â© 2025


